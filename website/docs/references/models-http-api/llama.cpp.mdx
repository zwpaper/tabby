import Collapse from '@site/src/components/Collapse';

# llama.cpp

[llama.cpp](https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md#api-endpoints) is a popular C++ library for serving gguf-based models. It provides a server implementation that supports completion, chat, and embedding functionalities through HTTP APIs.

## Chat model

llama.cpp provides an OpenAI-compatible chat API interface.

```toml title="~/.tabby/config.toml"
[[endpoints]]
name = "llamacpp_chat"
api_route = "http://localhost:8888"
metadata = {
  pochi = {
    use_case = "chat",
    provider = "openai",
    models = [
      { name = "your_model", context_window = 4096 }
    ]
  }
}
```

{/* FIXME(wei) update Completion config */}
## Completion model

llama.cpp offers a specialized completion API interface for code completion tasks.

```toml title="~/.tabby/config.toml"
[[endpoints]]
name = "llamacpp_completion"
api_route = "http://localhost:8888"
metadata = {
  pochi = {
    use_case = "completion",
    provider = "openai",
    models = [
      { name = "your_model", context_window = 4096 }
    ]
  }
}
```
